{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set BASE = /home/newdisk/btsun/project/PSR-SCI\n",
      "use sdp attention as default\n",
      "keep default attention mode\n",
      "use sdp attention as default\n",
      "keep default attention mode\n"
     ]
    }
   ],
   "source": [
    "# --- Setup base dir & env, then import deps (condensed) ---\n",
    "\n",
    "# Stdlib\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def get_basedir(up: int = 2) -> Path:\n",
    "    \"\"\"Return dir `up` levels above running .py/.ipynb.\"\"\"\n",
    "    try:\n",
    "        p = Path(__file__).resolve()        # .py\n",
    "    except NameError:\n",
    "        try:\n",
    "            import ipynbname                # notebook\n",
    "            p = Path(ipynbname.path()).resolve()\n",
    "        except Exception:\n",
    "            p = (Path.cwd() / \"_dummy\").resolve()    # fallback\n",
    "    for _ in range(up): p = p.parent\n",
    "    return p\n",
    "\n",
    "# only initialize BASE_DIR once\n",
    "BASE_DIR = globals().get(\"BASE_DIR\")\n",
    "if not isinstance(BASE_DIR, Path) or not BASE_DIR.exists():\n",
    "    BASE_DIR = get_basedir()\n",
    "\n",
    "sys.path[:0] = [\n",
    "    str(BASE_DIR),\n",
    "    str(BASE_DIR / \"packages\"),\n",
    "    str(BASE_DIR / \"packages\" / \"DiffBIR\"),\n",
    "    str(BASE_DIR / \"packages\" / \"MST\" / \"simulation\" / \"train_code\"),\n",
    "]\n",
    "os.chdir(BASE_DIR)\n",
    "print(f\"Set BASE = {BASE_DIR}\")\n",
    "os.environ.update({\"CUDA_DEVICE_ORDER\": \"PCI_BUS_ID\", \"CUDA_VISIBLE_DEVICES\": \"1\"})\n",
    "\n",
    "# Third-party\n",
    "import numpy as np, torch, scipy.io as scio, pytorch_lightning as pl\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from typing import Optional, Tuple, Set, List, Dict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from accelerate.utils import set_seed\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project\n",
    "from packages.DiffBIR.utils.common import (\n",
    "    instantiate_from_config, load_file_from_url, count_vram_usage,\n",
    "    wavelet_decomposition, wavelet_reconstruction, wavelet_decomposition_msi,\n",
    ")\n",
    "from packages.DiffBIR.utils.inference import InferenceLoop\n",
    "from packages.DiffBIR.utils.helpers import MSI_Pipeline\n",
    "from packages.DiffBIR.utils.cond_fn import MeasMSEGuidance, Guidance\n",
    "from packages.DiffBIR.model.gaussian_diffusion import Diffusion\n",
    "from packages.DiffBIR.model.cldm import ControlLDM\n",
    "from packages.MST.simulation.train_code.utils import *\n",
    "from packages.MST.simulation.train_code.architecture import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((2, 2))\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction_ratio, in_channels, 2, bias=False)\n",
    "\n",
    "        self.SiLU = nn.SiLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.SiLU(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.SiLU(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class DoubleConvWoBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x) + self.res_conv(x)\n",
    "\n",
    "\n",
    "class ChannelEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChannelEncoder, self).__init__()\n",
    "        self.conv1 = DoubleConvWoBN(in_channels=28, out_channels=21)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "            DoubleConvWoBN(in_channels=21, out_channels=9),\n",
    "        )\n",
    "        self.conv3 = DoubleConvWoBN(in_channels=9, out_channels=3)\n",
    "        self.conv_out = DoubleConvWoBN(in_channels=3, out_channels=3)\n",
    "        self.conv_res = nn.Sequential(\n",
    "            DoubleConvWoBN(in_channels=28, out_channels=3),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "        )\n",
    "\n",
    "        self.ca1 = ChannelAttention(28, 2)\n",
    "        self.ca2 = ChannelAttention(21, 2)\n",
    "        self.ca3 = ChannelAttention(9, 2)\n",
    "        self.ca_res = ChannelAttention(28, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = self.conv_res(x * self.ca_res(x))\n",
    "\n",
    "        x = x * self.ca1(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = x * self.ca2(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x * self.ca3(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.conv_out(x + res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChannelDecoder, self).__init__()\n",
    "        self.conv1 = DoubleConvWoBN(in_channels=3, out_channels=9)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=9, out_channels=21, kernel_size=2, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            DoubleConvWoBN(in_channels=21, out_channels=21),\n",
    "\n",
    "        )\n",
    "        self.conv3 = DoubleConvWoBN(in_channels=21, out_channels=28)\n",
    "        self.conv_out = DoubleConvWoBN(in_channels=28, out_channels=28)\n",
    "        self.conv_res = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, stride=2),\n",
    "            DoubleConvWoBN(in_channels=3, out_channels=28),\n",
    "        )\n",
    "\n",
    "        self.ca3 = ChannelAttention(28, 2)\n",
    "        self.ca2 = ChannelAttention(21, 2)\n",
    "        self.ca1 = ChannelAttention(9, 2)\n",
    "\n",
    "        self.ca_res = ChannelAttention(28, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = self.conv_res(x)\n",
    "        res = res * self.ca_res(res)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = x * self.ca1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = x * self.ca2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = x * self.ca3(x)\n",
    "\n",
    "        x = self.conv_out(x + res)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChannelVAE, self).__init__()\n",
    "        self.encoder = ChannelEncoder()\n",
    "        self.decoder = ChannelDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        en = self.encoder(x)\n",
    "        return self.decoder(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "# 删除已经存在的类定义以避免冲突\n",
    "if \"MeasMSEGuidance\" in globals():\n",
    "    del MeasMSEGuidance\n",
    "\n",
    "\n",
    "class MeasMSEGuidance(Guidance):\n",
    "    def load_guidance(self, target: torch.Tensor, masks: torch.Tensor, max_val_channel, min_val_channel, inputs_msi_lf, decoder: torch.nn.Module) -> None:\n",
    "        self.target = target\n",
    "        self.mask3d_batch = masks\n",
    "        self.max_val_channel, self.min_val_channel, self.inputs_msi_lf = max_val_channel, min_val_channel, inputs_msi_lf\n",
    "        self.decoder = decoder\n",
    "        self.rgb_target = None\n",
    "        self.bias = 0\n",
    "        self.lambda_reg = 0.005\n",
    "\n",
    "    def load_bias(self, bias: torch.Tensor):\n",
    "        self.bias = bias\n",
    "\n",
    "    def load_rgb_target(self, rgb_target: torch.Tensor):\n",
    "        self.rgb_target = rgb_target\n",
    "\n",
    "    def _forward(self, target: torch.Tensor, pred_x0: torch.Tensor, t: int, visual:bool=False) -> Tuple[torch.Tensor, float]:\n",
    "        # Ensure the directory exists\n",
    "        with torch.enable_grad():\n",
    "            pred_x0.requires_grad_(True)\n",
    "            pred_x0 = (pred_x0) / 2 + self.bias\n",
    "            # Clamp pred_x0 to [0, 1] range\n",
    "            pred_x0_clamped = torch.clamp(pred_x0, 0, 1)\n",
    "\n",
    "            def shift(inputs, step=2):\n",
    "                [bs, nC, row, col] = inputs.shape\n",
    "                output = torch.zeros(bs, nC, row, col + (nC - 1) * step).cuda().float()\n",
    "                for i in range(nC):\n",
    "                    output[:, i, :, step * i : step * i + col] = inputs[:, i, :, :]\n",
    "                return output\n",
    "\n",
    "            def gen_meas_torch(data_batch, mask3d_batch):\n",
    "                temp = shift(mask3d_batch * data_batch, 2)\n",
    "                meas = torch.sum(temp, 1)\n",
    "                return meas\n",
    "\n",
    "            # Calculate meas and loss\n",
    "            loss = 0\n",
    "            msi = self.decoder((pred_x0_clamped) * (self.max_val_channel - self.min_val_channel) + self.min_val_channel) + self.inputs_msi_lf\n",
    "\n",
    "            penalty_msi_low = torch.relu(-msi)  # penalize values below 0\n",
    "            msi_clamped = torch.clamp(msi, 0, 10)\n",
    "\n",
    "            meas = gen_meas_torch(msi_clamped, self.mask3d_batch)\n",
    "\n",
    "            meas = meas[:, :, 64:-64]\n",
    "            target = target[:, :, 64:-64]\n",
    "\n",
    "            target_mean = target.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)  # Calculate mean across height and width\n",
    "            meas_mean = meas.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)  # Calculate mean of meas\n",
    "\n",
    "            meas = meas * (target_mean / meas_mean * 0.5 + 1) /1.5\n",
    "\n",
    "            # Calculate the regularization term for out-of-bound values\n",
    "            penalty_low = torch.relu(-pred_x0)  # penalize values below 0\n",
    "            penalty_high = torch.relu(pred_x0 - 1)  # penalize values above 1\n",
    "            regularization = penalty_low.mean((1, 2, 3)).sum() + penalty_high.mean((1, 2, 3)).sum() + penalty_msi_low.mean((1, 2)).sum()\n",
    "\n",
    "            # Add regularization to the loss\n",
    "            loss += (meas - target).abs().mean((1, 2)).sum()\n",
    "            loss += self.lambda_reg * regularization  # lambda_reg is a weighting factor for the regularization term\n",
    "\n",
    "            if self.rgb_target is not None and self.rgb_subscale > 0:\n",
    "                loss += (pred_x0_clamped[:, :, :, 128:-128] - self.rgb_target[:, :, :, 128:-128]).pow(2).mean((1, 2, 3)).sum() * self.rgb_subscale\n",
    "\n",
    "        scale = self.scale\n",
    "        g = -torch.autograd.grad(loss, pred_x0)[0] * scale\n",
    "\n",
    "        if visual and t % 4 == 1:\n",
    "            visual_dir = \"visual/\"\n",
    "            os.makedirs(visual_dir, exist_ok=True)\n",
    "            with torch.no_grad():\n",
    "                # Prepare numpy arrays for pred_x0, meas, target, and difference (meas - target)\n",
    "                pred_x0_np = np.transpose(pred_x0_clamped.detach().cpu().numpy()[0], (1, 2, 0))  # HWC format\n",
    "                meas_np = meas.detach().cpu().numpy()[0]\n",
    "                target_np = target.detach().cpu().numpy()[0]\n",
    "                diff_np = (meas - target).detach().cpu().numpy()[0]\n",
    "\n",
    "                # Set up subplots\n",
    "                fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "                images = [(pred_x0_np, \"pred_x0\", None), (meas_np, \"meas\", \"gray\"), (target_np, \"target\", \"gray\"), (diff_np, \"meas - target\", \"coolwarm\")]  # pred_x0 image (no colorbar)  # meas image (grayscale)  # target image (grayscale)  # Difference image with colormap\n",
    "\n",
    "                # Display images\n",
    "                for ax, (img, title, cmap) in zip(axs, images):\n",
    "                    im = ax.imshow(img, cmap=cmap, vmin=0 if title in [\"meas\", \"target\"] else -0.4, vmax=10 if title in [\"meas\", \"target\"] else 0.4)\n",
    "                    ax.set_title(title)\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "                # Share a single colorbar between meas and target\n",
    "                cax = fig.add_axes([0.35, 0.1, 0.3, 0.03])  # Position for shared colorbar\n",
    "                fig.colorbar(axs[1].get_images()[0], cax=cax, orientation=\"horizontal\", label=\"Intensity (0-10)\")\n",
    "\n",
    "                # Save the combined figure\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visual_dir, f\"{t}_combined.png\"), bbox_inches=\"tight\", pad_inches=0)\n",
    "                plt.close()\n",
    "\n",
    "        return g, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    # stage_1 model weights\n",
    "    \"bsrnet\": \"https://github.com/cszn/KAIR/releases/download/v1.0/BSRNet.pth\",\n",
    "    # the following checkpoint is up-to-date, but we use the old version in our paper\n",
    "    # \"swinir_face\": \"https://github.com/zsyOAOA/DifFace/releases/download/V1.0/General_Face_ffhq512.pth\",\n",
    "    \"swinir_face\": \"https://huggingface.co/lxq007/DiffBIR/resolve/main/face_swinir_v1.ckpt\",\n",
    "    \"scunet_psnr\": \"https://github.com/cszn/KAIR/releases/download/v1.0/scunet_color_real_psnr.pth\",\n",
    "    \"swinir_general\": \"https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt\",\n",
    "    # stage_2 model weights\n",
    "    \"sd_v21\": \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\",\n",
    "    \"v1_face\": \"https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v1_face.pth\",\n",
    "    \"v1_general\": \"https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v1_general.pth\",\n",
    "    \"v2\": \"https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v2.pth\"\n",
    "}\n",
    "\n",
    "\n",
    "def load_model_from_url(url: str) -> Dict[str, torch.Tensor]:\n",
    "    sd_path = load_file_from_url(url, model_dir=str(BASE_DIR)+\"/packages/DiffBIR/weights\")\n",
    "    sd = torch.load(sd_path, map_location=\"cpu\", weights_only=False)\n",
    "    if \"state_dict\" in sd:\n",
    "        sd = sd[\"state_dict\"]\n",
    "    if list(sd.keys())[0].startswith(\"module\"):\n",
    "        sd = {k[len(\"module.\"):]: v for k, v in sd.items()}\n",
    "    return sd\n",
    "\n",
    "\n",
    "class InferenceLoop_NoPre:\n",
    "\n",
    "    def __init__(self, args: Namespace) -> \"InferenceLoop\":\n",
    "        self.args = args\n",
    "        self.loop_ctx = {}\n",
    "        self.pipeline: MSI_Pipeline = None\n",
    "        self.init_stage2_model()\n",
    "\n",
    "    @count_vram_usage\n",
    "    def init_stage2_model(self) -> None:\n",
    "        # load uent, vae, clip\n",
    "        self.cldm: ControlLDM = instantiate_from_config(OmegaConf.load(str(BASE_DIR)+\"/packages/DiffBIR/configs/inference/cldm.yaml\"))\n",
    "        sd = load_model_from_url(MODELS[\"sd_v21\"])\n",
    "        unused = self.cldm.load_pretrained_sd(sd)\n",
    "        print(f\"strictly load pretrained sd_v2.1, unused weights: {unused}\")\n",
    "        # load controlnet\n",
    "        self.cldm.load_controlnet_from_ckpt(torch.load(self.args.ckpt, map_location=\"cpu\"))\n",
    "        print(f\"strictly load controlnet weight {self.args.ckpt}\")\n",
    "        if self.args.vae != None:\n",
    "            self.cldm.load_vae_from_ckpt(torch.load(self.args.vae, map_location=\"cpu\"))\n",
    "            print(f\"strictly load vae weight {self.args.vae}\")\n",
    "        self.cldm.eval().cuda()\n",
    "        # load diffusion\n",
    "        self.diffusion: Diffusion = instantiate_from_config(OmegaConf.load(str(BASE_DIR)+\"/packages/DiffBIR/configs/inference/diffusion.yaml\"))\n",
    "        self.diffusion.cuda()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run(self, images: torch.tensor) -> torch.tensor:\n",
    "        # We don't support batch processing since input images may have different size\n",
    "\n",
    "        return self.pipeline.run_stage2(\n",
    "            images, self.args.steps, 1.0, self.args.tiled,\n",
    "            self.args.tile_size, self.args.tile_stride,\n",
    "            self.args.pos_prompt, self.args.neg_prompt, self.args.cfg_scale,\n",
    "            self.args.better_start\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def preprocess_data(input_meas: torch.tensor, input_mask: torch.tensor, DiffSCI_Pipeline: InferenceLoop_NoPre) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Preprocess input data by applying necessary transformations and normalization.\n",
    "\n",
    "    Args:\n",
    "        input_meas (torch.tensor): Input measurement data.\n",
    "        input_mask (torch.tensor): Input mask data.\n",
    "        model (ControlLDM): Model instance.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.tensor, torch.tensor]: Tuple containing preprocessed RGB images and normalization coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = input_meas.shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        MSI_IMAGE = DiffSCI_Pipeline.MSI_model(input_meas, input_mask)\n",
    "        inputs_msi_hf, inputs_msi_lf = wavelet_decomposition_msi(MSI_IMAGE, 3)\n",
    "\n",
    "        MSI_images_encoded = DiffSCI_Pipeline.encoder(inputs_msi_hf)\n",
    "\n",
    "        RANGE_MAX = 0.85\n",
    "        RANGE_MIN = 0.15\n",
    "\n",
    "        range_channel = torch.tensor([MSI_images_encoded[i].max() - MSI_images_encoded[i].min() for i in range(n_samples)]).cuda()\n",
    "        max_val_channel = torch.tensor([MSI_images_encoded[i].max() + range_channel[i] / (RANGE_MAX - RANGE_MIN)*(1-RANGE_MAX) for i in range(n_samples)]).cuda().view(n_samples, 1, 1, 1)\n",
    "        min_val_channel = torch.tensor([MSI_images_encoded[i].min() - range_channel[i] / (RANGE_MAX - RANGE_MIN)*(RANGE_MIN) for i in range(n_samples)]).cuda().view(n_samples, 1, 1, 1)\n",
    "\n",
    "        normalized_images = (MSI_images_encoded - min_val_channel) / (max_val_channel - min_val_channel)\n",
    "\n",
    "        return normalized_images, max_val_channel, min_val_channel, inputs_msi_lf\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_diffusion(\n",
    "    DiffSCI_Pipeline: InferenceLoop_NoPre,\n",
    "    normalized_images: torch.tensor,\n",
    "    max_val_channel: torch.tensor,\n",
    "    min_val_channel: torch.tensor,\n",
    "    inputs_msi_lf: torch.tensor,\n",
    "    steps: int,\n",
    "    upscale: int,\n",
    "    cfg_scale: float,\n",
    "    cond_fn: Optional[MeasMSEGuidance],\n",
    "    tiled: bool,\n",
    "    tile_size: int,\n",
    "    tile_stride: int,\n",
    "    better_start: bool = False,\n",
    "    pos_prompt: str = \"\",\n",
    "    neg_prompt: str = \"low quality, blurry, low-resolution, noisy, unsharp, weird textures\",\n",
    ") -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Apply Diffusion model on preprocessed data to generate restoration results.\n",
    "\n",
    "    Args:\n",
    "        model (ControlLDM): Model.\n",
    "        normalized_images (torch.tensor): Preprocessed normalized images.\n",
    "        max_val_channel (torch.tensor): Maximum values for each channel.\n",
    "        min_val_channel (torch.tensor): Minimum values for each channel.\n",
    "        steps (int): Sampling steps.\n",
    "        strength (float): Control strength.\n",
    "        color_fix_type (str): Type of color correction for samples.\n",
    "        cond_fn (Guidance | None): Guidance function.\n",
    "        tiled (bool): If True, a patch-based sampling strategy will be used.\n",
    "        tile_size (int): Size of patch.\n",
    "        tile_stride (int): Stride of sliding patch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.tensor, torch.tensor]: Tuple containing restored images and diffusion outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    if upscale > 1.0:\n",
    "        normalized_images_up = F.interpolate(normalized_images, size=(normalized_images.shape[-2] * upscale, normalized_images.shape[-1] * upscale), mode=\"bicubic\", antialias=True)\n",
    "    else:\n",
    "        normalized_images_up = normalized_images\n",
    "\n",
    "    DiffSCI_Pipeline.pipeline = MSI_Pipeline(DiffSCI_Pipeline.cldm, DiffSCI_Pipeline.diffusion, cond_fn, DiffSCI_Pipeline.args.device)\n",
    "\n",
    "    diffusion_output = DiffSCI_Pipeline.pipeline.run_stage2(\n",
    "        clean=normalized_images_up, steps=steps, strength=1.0, upscale=upscale, tiled=tiled, tile_size=tile_size, tile_stride=tile_stride,\n",
    "        pos_prompt=pos_prompt, neg_prompt=neg_prompt, cfg_scale=cfg_scale, better_start=better_start\n",
    "    )\n",
    "\n",
    "    if upscale > 1.0:\n",
    "        diffusion_output = F.interpolate(\n",
    "            diffusion_output,\n",
    "            size=(normalized_images.shape[-2], normalized_images.shape[-1]),\n",
    "            mode=\"bicubic\", antialias=True\n",
    "        )\n",
    "\n",
    "    diffusion_outputs = diffusion_output.contiguous().clamp(0, 1)\n",
    "\n",
    "    restored_images = DiffSCI_Pipeline.decoder(diffusion_outputs * (max_val_channel - min_val_channel) + min_val_channel) + inputs_msi_lf\n",
    "\n",
    "    return restored_images, diffusion_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "building SDPAttnBlock (sdp) with 512 in_channels\n",
      "building SDPAttnBlock (sdp) with 512 in_channels\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up SDPCrossAttention (sdp). Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "strictly load pretrained sd_v2.1, unused weights: {'sqrt_recipm1_alphas_cumprod', 'alphas_cumprod_prev', 'betas', 'log_one_minus_alphas_cumprod', 'posterior_mean_coef1', 'model_ema.num_updates', 'sqrt_recip_alphas_cumprod', 'alphas_cumprod', 'posterior_variance', 'model_ema.decay', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'posterior_mean_coef2', 'posterior_log_variance_clipped'}\n",
      "strictly load controlnet weight /home/newdisk/btsun/project/PSR-SCI/weights/controlnet_sample0160000.pt\n",
      "strictly load vae weight /home/newdisk/btsun/project/PSR-SCI/weights/vae_sample0012000.pt\n"
     ]
    }
   ],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser()\n",
    "    # model parameters\n",
    "    parser.add_argument(\"--ckpt\", type=str, default=str(BASE_DIR)+\"/weights/controlnet_sample0160000.pt\")\n",
    "    parser.add_argument(\"--vae\", type=str, default=str(BASE_DIR)+\"/weights/vae_sample0012000.pt\")\n",
    "    parser.add_argument(\"--channel_vae\", type=str, default=str(BASE_DIR)+\"/weights/model_SeVAE_hf3_endecoder_c21_bu2_c9_DConvWoBN_resca_silu_2024-09-05_psnr49.5199.pt\")\n",
    "    # sampling parameters\n",
    "    parser.add_argument(\"--steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--better_start\", type=bool, default=True)\n",
    "    parser.add_argument(\"--upscale\", type=int, default=1.0)\n",
    "    parser.add_argument(\"--tiled\", type=bool, default=False)\n",
    "    parser.add_argument(\"--tile_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--tile_stride\", type=int, default=256)\n",
    "    parser.add_argument(\"--pos_prompt\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--neg_prompt\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--cfg_scale\", type=float, default=1.0)\n",
    "    # input parameters\n",
    "    parser.add_argument(\"--n_samples\", type=int, default=1)\n",
    "    # guidance parameters\n",
    "    parser.add_argument(\"--guidance\", type=bool, default=True)\n",
    "    parser.add_argument(\"--g_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--g_t_start\", type=int, default=400)\n",
    "    parser.add_argument(\"--g_t_stop\", type=int, default=-1)\n",
    "    parser.add_argument(\"--g_space\", type=str, default=\"rgb\")\n",
    "    parser.add_argument(\"--g_repeat\", type=int, default=1)\n",
    "    # output parameters\n",
    "    # common parameters\n",
    "    parser.add_argument(\"--seed\", type=int, default=231)\n",
    "    parser.add_argument(\"--output\", type=str, default=\"./results/\")\n",
    "    parser.add_argument(\"--num_evals\", type=int, default=300)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = parse_args()\n",
    "args.device = torch.device(args.device)\n",
    "set_seed(args.seed)\n",
    "\n",
    "PSRSCI_Pipeline = InferenceLoop_NoPre(args=args)\n",
    "\n",
    "# 加载模型\n",
    "SeVAE_model = ChannelVAE()\n",
    "SeVAE_model = torch.load(args.channel_vae, map_location=\"cpu\" ,weights_only=False)\n",
    "SeVAE_model.eval().cuda()\n",
    "PSRSCI_Pipeline.encoder = SeVAE_model.encoder\n",
    "PSRSCI_Pipeline.decoder = SeVAE_model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始计算FLOPs...\n",
      "==================================================\n",
      "\n",
      "[1/3] 计算VAE编码的FLOPs...\n",
      "VAE Encode - FLOPs: 2.165T, Params: 34.147M\n",
      "VAE Encode - Latent shape: torch.Size([1, 4, 128, 128])\n",
      "\n",
      "[2/3] 计算Diffusion的FLOPs (10步*10)...\n",
      "Diffusion (单步) - FLOPs: 1\n",
      "Diffusion (100步) - FLOPs: 178.577T, Params: 1.229G\n",
      "  - ControlNet FLOPs: 4\n",
      "  - UNet FLOPs: 1\n",
      "\n",
      "[3/3] 计算VAE解码的FLOPs...\n",
      "VAE Decode - FLOPs: 4.960T, Params: 49.467M\n",
      "\n",
      "==================================================\n",
      "FLOPs 统计总结\n",
      "==================================================\n",
      "\n",
      "输入图像尺寸: 1x3x1024x1024\n",
      "Diffusion步数: 100步\n",
      "\n",
      "各模块FLOPs:\n",
      "  1. VAE编码:                2.165T (1.17%)\n",
      "  2. Diffusion:            178.577T (96.16%)\n",
      "  3. VAE解码:                4.960T (2.67%)\n",
      "\n",
      "总计:\n",
      "  总FLOPs:         185.702T\n",
      "  总参数量:        1.313G\n",
      "\n",
      "每步Diffusion的FLOPs: 1\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from thop import profile, clever_format\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 创建测试输入\n",
    "input_image = torch.zeros((1, 3, 1024, 1024), dtype=torch.float32, device=args.device)\n",
    "pos_prompt = [\"\"]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"开始计算FLOPs...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==================== 1. VAE编码 FLOPs ====================\n",
    "print(\"\\n[1/3] 计算VAE编码的FLOPs...\")\n",
    "with torch.no_grad():\n",
    "    # 准备VAE编码输入\n",
    "    vae_input = input_image * 2 - 1  # 归一化到[-1, 1]\n",
    "\n",
    "    # 计算VAE编码的FLOPs\n",
    "    flops_vae_encode, params_vae_encode = profile(\n",
    "        PSRSCI_Pipeline.cldm.vae.encoder,\n",
    "        inputs=(vae_input,),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 实际执行一次获取输出shape\n",
    "    z_encoded = PSRSCI_Pipeline.cldm.vae_encode(input_image, sample=False)\n",
    "\n",
    "flops_vae_encode_format, params_vae_encode_format = clever_format([flops_vae_encode, params_vae_encode], \"%.3f\")\n",
    "print(f\"VAE Encode - FLOPs: {flops_vae_encode_format}, Params: {params_vae_encode_format}\")\n",
    "print(f\"VAE Encode - Latent shape: {z_encoded.shape}\")\n",
    "\n",
    "# ==================== 2. Diffusion FLOPs (10步然后乘以10) ====================\n",
    "print(\"\\n[2/3] 计算Diffusion的FLOPs (10步*10)...\")\n",
    "\n",
    "# 准备condition\n",
    "with torch.no_grad():\n",
    "    c_txt = PSRSCI_Pipeline.cldm.clip.encode(pos_prompt)\n",
    "    c_img = z_encoded\n",
    "    cond = {\"c_txt\": c_txt, \"c_img\": c_img}\n",
    "\n",
    "    # 准备diffusion输入\n",
    "    bs, _, h, w = z_encoded.shape\n",
    "    x_noisy = torch.randn((bs, 4, h, w), dtype=torch.float32, device=args.device)\n",
    "    t = torch.tensor([500], dtype=torch.long, device=args.device)\n",
    "\n",
    "    # 设置control scales\n",
    "    PSRSCI_Pipeline.cldm.control_scales = [1.0] * 13\n",
    "\n",
    "    # 计算单步diffusion的FLOPs\n",
    "    # 先计算controlnet\n",
    "    flops_controlnet, params_controlnet = profile(\n",
    "        PSRSCI_Pipeline.cldm.controlnet,\n",
    "        inputs=(x_noisy, c_img, t, c_txt),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 执行controlnet获取control输出\n",
    "    control = PSRSCI_Pipeline.cldm.controlnet(x=x_noisy, hint=c_img, timesteps=t, context=c_txt)\n",
    "    control = [c * scale for c, scale in zip(control, PSRSCI_Pipeline.cldm.control_scales)]\n",
    "\n",
    "    # 计算unet\n",
    "    flops_unet, params_unet = profile(\n",
    "        PSRSCI_Pipeline.cldm.unet,\n",
    "        inputs=(x_noisy, t, c_txt, control, False),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 单步diffusion总FLOPs\n",
    "    flops_diffusion_single = flops_controlnet + flops_unet\n",
    "\n",
    "    # 100步的总FLOPs (通过10步乘以10估算)\n",
    "    flops_diffusion_total = flops_diffusion_single * 10 * 10\n",
    "    params_diffusion = params_controlnet + params_unet\n",
    "\n",
    "flops_diffusion_format, params_diffusion_format = clever_format([flops_diffusion_total, params_diffusion], \"%.3f\")\n",
    "flops_single_format = clever_format([flops_diffusion_single], \"%.3f\")[0]\n",
    "\n",
    "print(f\"Diffusion (单步) - FLOPs: {flops_single_format}\")\n",
    "print(f\"Diffusion (100步) - FLOPs: {flops_diffusion_format}, Params: {params_diffusion_format}\")\n",
    "print(f\"  - ControlNet FLOPs: {clever_format([flops_controlnet * 100], '%.3f')[0]}\")\n",
    "print(f\"  - UNet FLOPs: {clever_format([flops_unet * 100], '%.3f')[0]}\")\n",
    "\n",
    "# ==================== 3. VAE解码 FLOPs ====================\n",
    "print(\"\\n[3/3] 计算VAE解码的FLOPs...\")\n",
    "with torch.no_grad():\n",
    "    # 准备解码输入\n",
    "    z_to_decode = z_encoded / PSRSCI_Pipeline.cldm.scale_factor\n",
    "\n",
    "    # 计算VAE解码的FLOPs\n",
    "    flops_vae_decode, params_vae_decode = profile(\n",
    "        PSRSCI_Pipeline.cldm.vae.decoder,\n",
    "        inputs=(z_to_decode,),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "flops_vae_decode_format, params_vae_decode_format = clever_format([flops_vae_decode, params_vae_decode], \"%.3f\")\n",
    "print(f\"VAE Decode - FLOPs: {flops_vae_decode_format}, Params: {params_vae_decode_format}\")\n",
    "\n",
    "# ==================== 总结 ====================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FLOPs 统计总结\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_flops = flops_vae_encode + flops_diffusion_total + flops_vae_decode\n",
    "total_params = params_vae_encode + params_diffusion + params_vae_decode\n",
    "\n",
    "total_flops_format, total_params_format = clever_format([total_flops, total_params], \"%.3f\")\n",
    "\n",
    "print(f\"\\n输入图像尺寸: 1x3x1024x1024\")\n",
    "print(f\"Diffusion步数: 100步\")\n",
    "print(f\"\\n各模块FLOPs:\")\n",
    "print(f\"  1. VAE编码:       {flops_vae_encode_format:>15} ({flops_vae_encode/total_flops*100:.2f}%)\")\n",
    "print(f\"  2. Diffusion:     {flops_diffusion_format:>15} ({flops_diffusion_total/total_flops*100:.2f}%)\")\n",
    "print(f\"  3. VAE解码:       {flops_vae_decode_format:>15} ({flops_vae_decode/total_flops*100:.2f}%)\")\n",
    "print(f\"\\n总计:\")\n",
    "print(f\"  总FLOPs:         {total_flops_format}\")\n",
    "print(f\"  总参数量:        {total_params_format}\")\n",
    "print(f\"\\n每步Diffusion的FLOPs: {flops_single_format}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始计算FLOPs...\n",
      "==================================================\n",
      "\n",
      "[1/3] 计算VAE编码的FLOPs...\n",
      "VAE Encode - FLOPs: 4878.951 GFLOPs\n",
      "VAE Encode - Latent shape: torch.Size([1, 4, 128, 128])\n",
      "\n",
      "[2/3] 计算Diffusion的FLOPs (10步*10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1074591/3431431284.py:20: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter_vae_encode = FlopCounterMode(PSRSCI_Pipeline.cldm.vae.encoder, display=False)\n",
      "/tmp/ipykernel_1074591/3431431284.py:46: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter_controlnet = FlopCounterMode(PSRSCI_Pipeline.cldm.controlnet, display=False)\n",
      "/tmp/ipykernel_1074591/3431431284.py:47: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter_unet = FlopCounterMode(PSRSCI_Pipeline.cldm.unet, display=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion (单步) - FLOPs: 633.405 GFLOPs\n",
      "Diffusion (100步) - FLOPs: 63340.536 GFLOPs\n",
      "  - ControlNet FLOPs: 16590.633 GFLOPs\n",
      "  - UNet FLOPs: 46749.902 GFLOPs\n",
      "\n",
      "[3/3] 计算VAE解码的FLOPs...\n",
      "VAE Decode - FLOPs: 10470.393 GFLOPs\n",
      "\n",
      "==================================================\n",
      "FLOPs 统计总结\n",
      "==================================================\n",
      "\n",
      "输入图像尺寸: 1x3x1024x1024\n",
      "Diffusion步数: 100步\n",
      "\n",
      "各模块FLOPs:\n",
      "  1. VAE编码:         4878.951 GFLOPs (  6.20%)\n",
      "  2. Diffusion:      63340.536 GFLOPs ( 80.49%)\n",
      "     - ControlNet:   16590.633 GFLOPs ( 21.08%)\n",
      "     - UNet:         46749.902 GFLOPs ( 59.41%)\n",
      "  3. VAE解码:        10470.393 GFLOPs ( 13.31%)\n",
      "\n",
      "总计:\n",
      "  总FLOPs:          78689.879 GFLOPs\n",
      "\n",
      "每步Diffusion的FLOPs: 633.405 GFLOPs\n",
      "\n",
      "==================================================\n",
      "\n",
      "结果字典:\n",
      "  vae_encode_gflops: 4878.951\n",
      "  diffusion_total_gflops: 63340.536\n",
      "  controlnet_gflops: 16590.633\n",
      "  unet_gflops: 46749.902\n",
      "  vae_decode_gflops: 10470.393\n",
      "  total_gflops: 78689.879\n",
      "  single_step_gflops: 633.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1074591/3431431284.py:92: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter_vae_decode = FlopCounterMode(PSRSCI_Pipeline.cldm.vae.decoder, display=False)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 创建测试输入\n",
    "input_image = torch.zeros((1, 3, 1024, 1024), dtype=torch.float32, device=args.device)\n",
    "pos_prompt = [\"\"]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"开始计算FLOPs...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==================== 1. VAE编码 FLOPs ====================\n",
    "print(\"\\n[1/3] 计算VAE编码的FLOPs...\")\n",
    "with torch.no_grad():\n",
    "    # 准备VAE编码输入\n",
    "    vae_input = input_image * 2 - 1  # 归一化到[-1, 1]\n",
    "\n",
    "    # 使用FlopCounterMode计算VAE编码的FLOPs\n",
    "    flop_counter_vae_encode = FlopCounterMode(PSRSCI_Pipeline.cldm.vae.encoder, display=False)\n",
    "    with flop_counter_vae_encode:\n",
    "        z_encoded = PSRSCI_Pipeline.cldm.vae_encode(input_image, sample=False)\n",
    "\n",
    "    flops_vae_encode = flop_counter_vae_encode.get_total_flops()\n",
    "    flops_vae_encode_giga = flops_vae_encode / 1e9\n",
    "\n",
    "print(f\"VAE Encode - FLOPs: {flops_vae_encode_giga:.3f} GFLOPs\")\n",
    "print(f\"VAE Encode - Latent shape: {z_encoded.shape}\")\n",
    "\n",
    "# ==================== 2. Diffusion FLOPs (10步然后乘以10) ====================\n",
    "print(\"\\n[2/3] 计算Diffusion的FLOPs (10步*10)...\")\n",
    "\n",
    "# 准备condition\n",
    "with torch.no_grad():\n",
    "    c_txt = PSRSCI_Pipeline.cldm.clip.encode(pos_prompt)\n",
    "    c_img = z_encoded\n",
    "    cond = {\"c_txt\": c_txt, \"c_img\": c_img}\n",
    "\n",
    "    # 准备diffusion输入\n",
    "    bs, _, h, w = z_encoded.shape\n",
    "\n",
    "    # 设置control scales\n",
    "    PSRSCI_Pipeline.cldm.control_scales = [1.0] * 13\n",
    "\n",
    "    # 计算10步diffusion的FLOPs\n",
    "    flop_counter_controlnet = FlopCounterMode(PSRSCI_Pipeline.cldm.controlnet, display=False)\n",
    "    flop_counter_unet = FlopCounterMode(PSRSCI_Pipeline.cldm.unet, display=False)\n",
    "\n",
    "    flops_controlnet_total = 0\n",
    "    flops_unet_total = 0\n",
    "\n",
    "    # 运行10步\n",
    "    for step in range(10):\n",
    "        x_noisy = torch.randn((bs, 4, h, w), dtype=torch.float32, device=args.device)\n",
    "        t = torch.tensor([step * 100], dtype=torch.long, device=args.device)\n",
    "\n",
    "        # 计算controlnet\n",
    "        with flop_counter_controlnet:\n",
    "            control = PSRSCI_Pipeline.cldm.controlnet(x=x_noisy, hint=c_img, timesteps=t, context=c_txt)\n",
    "\n",
    "        control = [c * scale for c, scale in zip(control, PSRSCI_Pipeline.cldm.control_scales)]\n",
    "\n",
    "        # 计算unet\n",
    "        with flop_counter_unet:\n",
    "            eps = PSRSCI_Pipeline.cldm.unet(x=x_noisy, timesteps=t, context=c_txt, control=control, only_mid_control=False)\n",
    "\n",
    "    flops_controlnet_10steps = flop_counter_controlnet.get_total_flops()\n",
    "    flops_unet_10steps = flop_counter_unet.get_total_flops()\n",
    "\n",
    "    # 估算100步的FLOPs\n",
    "    flops_controlnet_100steps = flops_controlnet_10steps * 10\n",
    "    flops_unet_100steps = flops_unet_10steps * 10\n",
    "    flops_diffusion_total = flops_controlnet_100steps + flops_unet_100steps\n",
    "\n",
    "    flops_controlnet_giga = flops_controlnet_100steps / 1e9\n",
    "    flops_unet_giga = flops_unet_100steps / 1e9\n",
    "    flops_diffusion_giga = flops_diffusion_total / 1e9\n",
    "    flops_single_step_giga = (flops_controlnet_10steps + flops_unet_10steps) / 10 / 1e9\n",
    "\n",
    "print(f\"Diffusion (单步) - FLOPs: {flops_single_step_giga:.3f} GFLOPs\")\n",
    "print(f\"Diffusion (100步) - FLOPs: {flops_diffusion_giga:.3f} GFLOPs\")\n",
    "print(f\"  - ControlNet FLOPs: {flops_controlnet_giga:.3f} GFLOPs\")\n",
    "print(f\"  - UNet FLOPs: {flops_unet_giga:.3f} GFLOPs\")\n",
    "\n",
    "# ==================== 3. VAE解码 FLOPs ====================\n",
    "print(\"\\n[3/3] 计算VAE解码的FLOPs...\")\n",
    "with torch.no_grad():\n",
    "    # 准备解码输入 - 使用随机latent而不是之前的结果\n",
    "    z_to_decode = torch.randn_like(z_encoded) / PSRSCI_Pipeline.cldm.scale_factor\n",
    "\n",
    "    # 使用FlopCounterMode计算VAE解码的FLOPs\n",
    "    flop_counter_vae_decode = FlopCounterMode(PSRSCI_Pipeline.cldm.vae.decoder, display=False)\n",
    "    with flop_counter_vae_decode:\n",
    "        decoded_image = PSRSCI_Pipeline.cldm.vae_decode(z_to_decode)\n",
    "\n",
    "    flops_vae_decode = flop_counter_vae_decode.get_total_flops()\n",
    "    flops_vae_decode_giga = flops_vae_decode / 1e9\n",
    "\n",
    "print(f\"VAE Decode - FLOPs: {flops_vae_decode_giga:.3f} GFLOPs\")\n",
    "\n",
    "# ==================== 总结 ====================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FLOPs 统计总结\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_flops = flops_vae_encode + flops_diffusion_total + flops_vae_decode\n",
    "total_flops_giga = total_flops / 1e9\n",
    "\n",
    "print(f\"\\n输入图像尺寸: 1x3x1024x1024\")\n",
    "print(f\"Diffusion步数: 100步\")\n",
    "print(f\"\\n各模块FLOPs:\")\n",
    "print(f\"  1. VAE编码:       {flops_vae_encode_giga:>10.3f} GFLOPs ({flops_vae_encode/total_flops*100:>6.2f}%)\")\n",
    "print(f\"  2. Diffusion:     {flops_diffusion_giga:>10.3f} GFLOPs ({flops_diffusion_total/total_flops*100:>6.2f}%)\")\n",
    "print(f\"     - ControlNet:  {flops_controlnet_giga:>10.3f} GFLOPs ({flops_controlnet_100steps/total_flops*100:>6.2f}%)\")\n",
    "print(f\"     - UNet:        {flops_unet_giga:>10.3f} GFLOPs ({flops_unet_100steps/total_flops*100:>6.2f}%)\")\n",
    "print(f\"  3. VAE解码:       {flops_vae_decode_giga:>10.3f} GFLOPs ({flops_vae_decode/total_flops*100:>6.2f}%)\")\n",
    "print(f\"\\n总计:\")\n",
    "print(f\"  总FLOPs:         {total_flops_giga:>10.3f} GFLOPs\")\n",
    "print(f\"\\n每步Diffusion的FLOPs: {flops_single_step_giga:.3f} GFLOPs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 可选：保存结果到字典\n",
    "flops_results = {\n",
    "    \"vae_encode_gflops\": flops_vae_encode_giga,\n",
    "    \"diffusion_total_gflops\": flops_diffusion_giga,\n",
    "    \"controlnet_gflops\": flops_controlnet_giga,\n",
    "    \"unet_gflops\": flops_unet_giga,\n",
    "    \"vae_decode_gflops\": flops_vae_decode_giga,\n",
    "    \"total_gflops\": total_flops_giga,\n",
    "    \"single_step_gflops\": flops_single_step_giga,\n",
    "}\n",
    "\n",
    "print(\"\\n结果字典:\")\n",
    "for key, value in flops_results.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各模块FLOPs:\n",
    "  1. VAE编码:                2.165T (1.17%)\n",
    "  2. Diffusion:            178.577T (96.16%)\n",
    "  3. VAE解码:                4.960T (2.67%)\n",
    "\n",
    "总计:\n",
    "  总FLOPs:         185.702T\n",
    "  总参数量:        1.313G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psrsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
